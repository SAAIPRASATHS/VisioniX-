# VisioniX - Statistella Round 2 ML Pipeline

## ğŸ† B.A.S.H Data Analytics Competition

A complete end-to-end Machine Learning pipeline for predicting **Importance Score (0-100)** for legal documents in the Statistella Round 2 competition.

---

## ğŸ“‹ Project Overview

This project implements a robust ML pipeline using **LightGBM** with extensive feature engineering to predict document importance scores based on textual and categorical features.

### Key Features

- **Text Feature Engineering**: TF-IDF vectorization on document titles, keywords, and descriptions
- **Categorical Encoding**: MultiLabel encoding for categorical variables
- **Count-based Features**: Text length, word counts, entity frequencies
- **Advanced Regression**: LightGBM with early stopping and hyperparameter tuning
- **Ensemble Model**: LightGBM + XGBoost ensemble (improved version)

---

## ğŸ“ Project Structure

```
VisioniX/
â”œâ”€â”€ bash-8-0-round-2/
â”‚   â”œâ”€â”€ train.csv              # Training dataset (20,624 samples)
â”‚   â””â”€â”€ test.csv               # Test dataset (5,157 samples)
â”œâ”€â”€ statistella_pipeline.py    # Main ML pipeline (LightGBM)
â”œâ”€â”€ statistella_improved.py    # Enhanced pipeline (LightGBM + XGBoost ensemble)
â”œâ”€â”€ statistella_notebook.ipynb # Kaggle notebook version
â”œâ”€â”€ submission.csv             # Kaggle submission file
â”œâ”€â”€ feature_importance.png     # Feature importance visualization
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

---

## ğŸš€ How to Run

### Prerequisites

```bash
pip install -r requirements.txt
```

### Option 1: Run Basic Pipeline

```bash
python statistella_pipeline.py
```

### Option 2: Run Improved Ensemble Pipeline

```bash
python statistella_improved.py
```

### Option 3: Use Kaggle Notebook

1. Upload `statistella_notebook.ipynb` to Kaggle
2. Add the competition dataset
3. Run all cells
4. Submit the generated `submission.csv`

---

## ğŸ“Š Model Performance

| Model | Validation RMSE |
|-------|-----------------|
| LightGBM (Basic) | ~4.04 |
| LightGBM + XGBoost Ensemble | ~3.95 |

---

## ğŸ”§ Tech Stack

- **Python 3.8+**
- **LightGBM** - Gradient Boosting Framework
- **XGBoost** - Extreme Gradient Boosting (Ensemble)
- **Pandas** - Data Manipulation
- **Scikit-learn** - TF-IDF & Preprocessing
- **NumPy** - Numerical Computing

---

## ğŸ“ˆ Feature Engineering Details

| Feature Type | Description | Count |
|--------------|-------------|-------|
| TF-IDF (Headline) | Unigrams & Bigrams | 500 |
| TF-IDF (Key Insights) | Unigrams & Bigrams | 1000 |
| TF-IDF (Reasoning) | Unigrams & Bigrams | 500 |
| TF-IDF (Tags) | Unigrams | 200 |
| MultiLabel (Lead Types) | Binary encoding | Variable |
| MultiLabel (Power Mentions) | Binary encoding | Variable |
| MultiLabel (Agencies) | Binary encoding | Variable |
| Count Features | Text lengths, word counts | 13 |

---

## ğŸ“ Submission Format

The output `submission.csv` follows the required format:

```csv
id,Importance Score
21292,4.35
16024,6.45
10203,12.04
...
```

- **id**: Document identifier
- **Importance Score**: Predicted value (0-100)

---

## ğŸ‘¨â€ğŸ’» Author

**SAAIPRASATH S**

---

## ğŸ“„ Competition

**Statistella â€“ B.A.S.H Round 2** | Kaggle Data Analytics Competition

---

â­ **Star this repo if you found it helpful!**
