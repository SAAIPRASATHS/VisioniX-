{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistella Round 2 - Importance Score Prediction\n",
    "## B.A.S.H Data Analytics Competition\n",
    "\n",
    "This notebook predicts **Importance Score (0-100)** for legal documents using:\n",
    "- TF-IDF text features\n",
    "- MultiLabel categorical encoding\n",
    "- LightGBM regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/bash-8-0-round-2/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/bash-8-0-round-2/test.csv')\n",
    "\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "print(\"\\nColumns:\", list(train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target 'Importance Score' Statistics:\")\n",
    "train['Importance Score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['Headline', 'Reasoning', 'Key Insights', 'Tags']\n",
    "list_cols = ['Lead Types', 'Power Mentions', 'Agencies']\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def parse_list_column(value):\n",
    "    if pd.isna(value) or value == '':\n",
    "        return []\n",
    "    items = re.split(r'[;,]', str(value))\n",
    "    return [item.strip().lower() for item in items if item.strip()]\n",
    "\n",
    "for col in text_cols:\n",
    "    train[col + '_clean'] = train[col].apply(clean_text)\n",
    "    test[col + '_clean'] = test[col].apply(clean_text)\n",
    "\n",
    "for col in list_cols:\n",
    "    train[col + '_list'] = train[col].apply(parse_list_column)\n",
    "    test[col + '_list'] = test[col].apply(parse_list_column)\n",
    "\n",
    "print(\"Data cleaned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headlines = pd.concat([train['Headline_clean'], test['Headline_clean']])\n",
    "all_insights = pd.concat([train['Key Insights_clean'], test['Key Insights_clean']])\n",
    "all_reasoning = pd.concat([train['Reasoning_clean'], test['Reasoning_clean']])\n",
    "all_tags = pd.concat([train['Tags_clean'], test['Tags_clean']])\n",
    "\n",
    "tfidf_headline = TfidfVectorizer(max_features=500, ngram_range=(1, 2), min_df=3)\n",
    "tfidf_insights = TfidfVectorizer(max_features=1000, ngram_range=(1, 2), min_df=3)\n",
    "tfidf_reasoning = TfidfVectorizer(max_features=500, ngram_range=(1, 2), min_df=3)\n",
    "tfidf_tags = TfidfVectorizer(max_features=200, ngram_range=(1, 1), min_df=3)\n",
    "\n",
    "tfidf_headline.fit(all_headlines)\n",
    "tfidf_insights.fit(all_insights)\n",
    "tfidf_reasoning.fit(all_reasoning)\n",
    "tfidf_tags.fit(all_tags)\n",
    "\n",
    "train_headline_tfidf = tfidf_headline.transform(train['Headline_clean'])\n",
    "test_headline_tfidf = tfidf_headline.transform(test['Headline_clean'])\n",
    "train_insights_tfidf = tfidf_insights.transform(train['Key Insights_clean'])\n",
    "test_insights_tfidf = tfidf_insights.transform(test['Key Insights_clean'])\n",
    "train_reasoning_tfidf = tfidf_reasoning.transform(train['Reasoning_clean'])\n",
    "test_reasoning_tfidf = tfidf_reasoning.transform(test['Reasoning_clean'])\n",
    "train_tags_tfidf = tfidf_tags.transform(train['Tags_clean'])\n",
    "test_tags_tfidf = tfidf_tags.transform(test['Tags_clean'])\n",
    "\n",
    "print(\"TF-IDF Features:\")\n",
    "print(f\"  Headline: {train_headline_tfidf.shape[1]}\")\n",
    "print(f\"  Key Insights: {train_insights_tfidf.shape[1]}\")\n",
    "print(f\"  Reasoning: {train_reasoning_tfidf.shape[1]}\")\n",
    "print(f\"  Tags: {train_tags_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lead_types = train['Lead Types_list'].tolist() + test['Lead Types_list'].tolist()\n",
    "all_power_mentions = train['Power Mentions_list'].tolist() + test['Power Mentions_list'].tolist()\n",
    "all_agencies = train['Agencies_list'].tolist() + test['Agencies_list'].tolist()\n",
    "\n",
    "mlb_lead = MultiLabelBinarizer(sparse_output=True)\n",
    "mlb_power = MultiLabelBinarizer(sparse_output=True)\n",
    "mlb_agency = MultiLabelBinarizer(sparse_output=True)\n",
    "\n",
    "mlb_lead.fit(all_lead_types)\n",
    "mlb_power.fit(all_power_mentions)\n",
    "mlb_agency.fit(all_agencies)\n",
    "\n",
    "train_lead_enc = mlb_lead.transform(train['Lead Types_list'])\n",
    "test_lead_enc = mlb_lead.transform(test['Lead Types_list'])\n",
    "train_power_enc = mlb_power.transform(train['Power Mentions_list'])\n",
    "test_power_enc = mlb_power.transform(test['Power Mentions_list'])\n",
    "train_agency_enc = mlb_agency.transform(train['Agencies_list'])\n",
    "test_agency_enc = mlb_agency.transform(test['Agencies_list'])\n",
    "\n",
    "print(\"Multilabel Features:\")\n",
    "print(f\"  Lead Types: {train_lead_enc.shape[1]}\")\n",
    "print(f\"  Power Mentions: {train_power_enc.shape[1]}\")\n",
    "print(f\"  Agencies: {train_agency_enc.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_features(df):\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    features['headline_len'] = df['Headline'].fillna('').apply(len)\n",
    "    features['reasoning_len'] = df['Reasoning'].fillna('').apply(len)\n",
    "    features['insights_len'] = df['Key Insights'].fillna('').apply(len)\n",
    "    features['tags_len'] = df['Tags'].fillna('').apply(len)\n",
    "    \n",
    "    features['headline_words'] = df['Headline'].fillna('').apply(lambda x: len(str(x).split()))\n",
    "    features['reasoning_words'] = df['Reasoning'].fillna('').apply(lambda x: len(str(x).split()))\n",
    "    features['insights_words'] = df['Key Insights'].fillna('').apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    features['lead_types_count'] = df['Lead Types_list'].apply(len)\n",
    "    features['power_mentions_count'] = df['Power Mentions_list'].apply(len)\n",
    "    features['agencies_count'] = df['Agencies_list'].apply(len)\n",
    "    \n",
    "    features['has_lead_types'] = (features['lead_types_count'] > 0).astype(int)\n",
    "    features['has_power_mentions'] = (features['power_mentions_count'] > 0).astype(int)\n",
    "    features['has_agencies'] = (features['agencies_count'] > 0).astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "train_counts = create_count_features(train)\n",
    "test_counts = create_count_features(test)\n",
    "print(f\"Count-based features: {train_counts.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts_sparse = csr_matrix(train_counts.values)\n",
    "test_counts_sparse = csr_matrix(test_counts.values)\n",
    "\n",
    "X_train = hstack([\n",
    "    train_headline_tfidf, train_insights_tfidf, train_reasoning_tfidf, train_tags_tfidf,\n",
    "    train_lead_enc, train_power_enc, train_agency_enc,\n",
    "    train_counts_sparse\n",
    "])\n",
    "\n",
    "X_test = hstack([\n",
    "    test_headline_tfidf, test_insights_tfidf, test_reasoning_tfidf, test_tags_tfidf,\n",
    "    test_lead_enc, test_power_enc, test_agency_enc,\n",
    "    test_counts_sparse\n",
    "])\n",
    "\n",
    "y_train = train['Importance Score'].values\n",
    "\n",
    "print(f\"Total Features: {X_train.shape[1]}\")\n",
    "print(f\"Training Samples: {X_train.shape[0]}\")\n",
    "print(f\"Test Samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training: {X_tr.shape[0]}\")\n",
    "print(f\"Validation: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 64,\n",
    "    'max_depth': 10,\n",
    "    'min_child_samples': 20,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.1,\n",
    "    'n_estimators': 2000,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "\n",
    "model = lgb.train(\n",
    "    lgb_params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train', 'valid'],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"\\nBest iteration: {model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(X_val)\n",
    "val_preds = np.clip(val_preds, 0, 100)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "test_preds = np.clip(test_preds, 0, 100)\n",
    "\n",
    "print(f\"Generated {len(test_preds)} predictions\")\n",
    "print(f\"Prediction range: [{test_preds.min():.2f}, {test_preds.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Importance Score': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved!\")\n",
    "print(f\"Shape: {submission.shape}\")\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n",
    "print(f\"Total features: {X_train.shape[1]}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"Best iteration: {model.best_iteration}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
